{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "from testCases import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n dimensional gradient checking\n",
    "\n",
    "def forward_prop(x, y, parameters):\n",
    "    cache = {}\n",
    "    w1 = parameters[\"W1\"]\n",
    "    w2 = parameters[\"W2\"]\n",
    "    w3 = parameters[\"W3\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    z1 = np.dot(w1,x)+b1\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "\n",
    "    z3 = np.dot(w3, a2) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    cache[\"z1\"] = z1\n",
    "    cache[\"z2\"] = z2\n",
    "    cache[\"z3\"] = z3\n",
    "    cache[\"a1\"] = a1\n",
    "    cache[\"a2\"] = a2\n",
    "    cache[\"a3\"] = a3\n",
    "\n",
    "    cost =  np.sum(np.multiply(y, -np.log(a3)) + np.multiply(1-y, - np.log(1-a3)))/x.shape[1]\n",
    "\n",
    "    return cache, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.407833389834357\n"
     ]
    }
   ],
   "source": [
    "cache, cost = forward_prop(*gradient_check_n_test_case())\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, y ,parameters , cache):\n",
    "    m = x.shape[1]\n",
    "    z1 = cache[\"z1\"]\n",
    "    z2 = cache[\"z2\"]\n",
    "    z3 = cache[\"z3\"]\n",
    "    a1 = cache[\"a1\"]          # 5,3\n",
    "    a2 = cache[\"a2\"]            #3,3\n",
    "    a3 = cache[\"a3\"]            #1,3\n",
    "\n",
    "    w1 = parameters[\"W1\"]\n",
    "    w2 = parameters[\"W2\"]\n",
    "    w3 = parameters[\"W3\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    dz3  = a3 -y        #1,3\n",
    "    da2 = np.dot(w3.T, dz3)        #3,3\n",
    "    #\n",
    "    dz2 = np.copy(da2)         #3,3\n",
    "    dz2[z2<=0] = 0 \n",
    "    #\n",
    "    da1 = np.dot(w2.T, dz2)         #5,3\n",
    "    #\n",
    "    dz1 = np.copy(da1)\n",
    "    dz1[z1<=0] = 0               #5,3\n",
    "\n",
    "    dw3 = 1./m*np.dot(dz3 , a2.T)    #1,3\n",
    "    db3 = 1./m * np.sum(dz3, axis=1, keepdims = True)   #1,1\n",
    "\n",
    "    dw2 = 1./m*np.dot(dz2, a1.T)   #3,5\n",
    "    db2 = 1./m * np.sum(dz2, axis=1, keepdims = True)   #3,1\n",
    "\n",
    "    dw1 = 1./m *np.dot(dz1, x.T)    #5,4\n",
    "    db1 = 1./m * np.sum(dz1, axis=1, keepdims = True)   #5,1\n",
    "\n",
    "    caches = {}\n",
    "    caches[\"dW3\"] = dw3\n",
    "    caches[\"dW2\"] = dw2\n",
    "    caches[\"dW1\"] = dw1\n",
    "    caches[\"db1\"] = db1\n",
    "    caches[\"db2\"] = db2\n",
    "    caches[\"db3\"] = db3\n",
    "    return caches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, caches , learning_rate):\n",
    "    for item in parameters.keys():\n",
    "        parameters[str(item)] = parameters[str(item)] - learning_rate*caches[str('d')+str(item)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(num_iterations, learning_rate):\n",
    "    x, y, parameters = gradient_check_n_test_case()\n",
    "    costs = []\n",
    "    for i in range (num_iterations):\n",
    "        cache, cost = forward_prop(x, y, parameters)\n",
    "        caches = backprop(x, y, parameters, cache)\n",
    "        parameters = update_parameters(parameters, caches, learning_rate)\n",
    "        if i%100==0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.407833389834357, 1.5687398899693203, 1.1386467367037953, 0.919205250718965, 0.8024691337687141, 0.734419456265721, 0.6899210989758918, 0.6569132533111964, 0.6291600798954751, 0.6031325200473504, 0.5766747041094027, 0.5484446891719149, 0.5325103415774931, 0.5207744543832705, 0.5091106833983515, 0.49753727803599607, 0.4862128517814133, 0.4751962335236111, 0.4644441399076767, 0.4540579433655954]\n"
     ]
    }
   ],
   "source": [
    "print(model(2000 , 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_gradient(cost, epsilon):\n",
    "\n",
    "def gradient_checking(x, y , parameters ,gradients ,  epsilon = 1e-3):\n",
    "    \n",
    "    grad = gradients_to_vector(gradients)\n",
    "    theta, _ = dictionary_to_vector(parameters)\n",
    "    num_parameters = theta.shape[0]\n",
    "    # gradients = []\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range (num_parameters):\n",
    "        temp_theta = np.copy(theta)\n",
    "        temp_theta[i][0] = temp_theta[i][0] + epsilon\n",
    "        temp_parameters = vector_to_dictionary(temp_theta)\n",
    "        _ , J_plus[i] = forward_prop(x,y, temp_parameters)\n",
    "\n",
    "        temp_theta = np.copy(theta)\n",
    "        temp_theta[i] = temp_theta[i] - epsilon\n",
    "        temp_parameters = vector_to_dictionary(temp_theta)\n",
    "        _ , J_minus[i] = forward_prop(x,y, temp_parameters)\n",
    "\n",
    "        gradapprox[i]= (J_plus[i] - J_minus[i])/(2*epsilon)\n",
    "        \n",
    "\n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    dinominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "\n",
    "    difference = numerator/dinominator\n",
    "\n",
    "    if difference>2e-7:\n",
    "        print(\"there is a problem in back propagation algorithm\")\n",
    "    else:\n",
    "        print(\"the back propagation is correctly implemented\")\n",
    "\n",
    "    return difference\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the back propagation is correctly implemented\n",
      "1.6797905539453286e-08 diff\n"
     ]
    }
   ],
   "source": [
    "x, y, parameters = gradient_check_n_test_case()\n",
    "cache , cost = forward_prop(x, y, parameters)\n",
    "gradients = backprop(x,y, parameters, cache)\n",
    "\n",
    "diff = gradient_checking(x, y, parameters, gradients)\n",
    "print(diff , \"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
