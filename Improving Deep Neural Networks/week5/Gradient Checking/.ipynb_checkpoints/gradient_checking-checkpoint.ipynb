{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "from testCases import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08851412]\n",
      " [ 0.00802506]\n",
      " [ 0.02282388]\n",
      " [-0.08804768]\n",
      " [ 0.02781288]]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# J(Q) = Qx\n",
    "\n",
    "def calculate_cost(theta, x):\n",
    "    cost = np.sum(theta*x)\n",
    "    return cost\n",
    "\n",
    "def backprop(x : np.ndarray):\n",
    "    dq = x\n",
    "    # theta = theta -dq\n",
    "    # return theta\n",
    "    return dq\n",
    "\n",
    "def gradient_checking(x:np.ndarray, theta:np.ndarray, epsilon:float)->np.ndarray:\n",
    "    dqarray = []\n",
    "    newtheta = np.copy(np.reshape(theta , (-1, 1)))\n",
    "    for i in range(len(newtheta)):\n",
    "        newtheta[i][0] = newtheta[i][0]+epsilon\n",
    "        dq = (calculate_cost(newtheta, x) - calculate_cost(newtheta, x))/(2*epsilon)\n",
    "        dqarray.append(dq)\n",
    "    return dqarray\n",
    "\n",
    "theta = np.random.rand(5,1)\n",
    "x = np.random.randn(5,1)*0.1\n",
    "\n",
    "print(backprop(x))\n",
    "print(gradient_checking(x, theta,  0.0001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n dimensional gradient checking\n",
    "\n",
    "def forward_prop(x, y, parameters):\n",
    "    cache = {}\n",
    "    w1 = parameters[\"W1\"]\n",
    "    w2 = parameters[\"W2\"]\n",
    "    w3 = parameters[\"W3\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    z1 = np.dot(w1,x)+b1\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "\n",
    "    z3 = np.dot(w3, a2) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    cache[\"z1\"] = z1\n",
    "    cache[\"z2\"] = z2\n",
    "    cache[\"z3\"] = z3\n",
    "    cache[\"a1\"] = a1\n",
    "    cache[\"a2\"] = a2\n",
    "    cache[\"a3\"] = a3\n",
    "\n",
    "    cost =  -1 *np.sum(np.multiply(y, np.log(a3)) + np.multiply((1-y), np.log(1-a3)))/x.shape[1]\n",
    "\n",
    "    return cache, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.407833389834357\n"
     ]
    }
   ],
   "source": [
    "cache, cost = forward_prop(*gradient_check_n_test_case())\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, y ,parameters , cache):\n",
    "    m = x.shape[1]\n",
    "    # z1 = cache[\"z1\"]\n",
    "    # z2 = cache[\"z2\"]\n",
    "    # z3 = cache[\"z3\"]\n",
    "    a1 = cache[\"a1\"]          # 5,3\n",
    "    a2 = cache[\"a2\"]            #3,3\n",
    "    a3 = cache[\"a3\"]            #1,3\n",
    "\n",
    "    # w1 = parameters[\"W1\"]\n",
    "    w2 = parameters[\"W2\"]\n",
    "    w3 = parameters[\"W3\"]\n",
    "    # b1 = parameters[\"b1\"]\n",
    "    # b2 = parameters[\"b2\"]\n",
    "    # b3 = parameters[\"b3\"]\n",
    "\n",
    "    dz3  = a3 -y        #1,3\n",
    "    da2 = np.dot(w3.T, dz3)        #3,3\n",
    "    #\n",
    "    dz2 = np.copy(da2)         #3,3\n",
    "    dz2[a2<0] = 0 \n",
    "    #\n",
    "    da1 = np.dot(w2.T, dz2)         #5,3\n",
    "    #\n",
    "    dz1 = np.copy(da1)\n",
    "    dz1[a1<0] = 0               #5,3\n",
    "\n",
    "    dw3 = 1./m*np.dot(dz3 , a2.T)    #1,3\n",
    "    db3 = 1./m * np.sum(dz3, axis=1, keepdims = True)   #1,1\n",
    "\n",
    "    dw2 = 1./m*np.dot(dz2, a1.T)   #3,5\n",
    "    db2 = 1./m * np.sum(dz2, axis=1, keepdims = True)   #3,1\n",
    "\n",
    "    dw1 = 1./m *np.dot(dz1, x.T)    #5,4\n",
    "    db1 = 1./m * np.sum(dz1, axis=1, keepdims = True)   #5,1\n",
    "\n",
    "    caches = {}\n",
    "    caches[\"dW3\"] = dw3\n",
    "    caches[\"dW2\"] = dw2\n",
    "    caches[\"dW1\"] = dw1\n",
    "    caches[\"db1\"] = db1\n",
    "    caches[\"db2\"] = db2\n",
    "    caches[\"db3\"] = db3\n",
    "    return caches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, caches , learning_rate):\n",
    "    for item in parameters.keys():\n",
    "        parameters[str(item)] = parameters[str(item)] - learning_rate*caches[str('d')+str(item)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(num_iterations, learning_rate):\n",
    "    x, y, parameters = gradient_check_n_test_case()\n",
    "    costs = []\n",
    "    for i in range (num_iterations):\n",
    "        cache, cost = forward_prop(x, y, parameters)\n",
    "        caches = backprop(x, y, parameters, cache)\n",
    "        parameters = update_parameters(parameters, caches, learning_rate)\n",
    "        if i%100==0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.407833389834357, 1.629788088653249, 1.2065248206319676, 0.9176299287776573, 0.7603297163871111, 0.6643490391175716, 0.5985556320693467, 0.548132034852938, 0.5060313357126032, 0.4692393492810551, 0.4366157608243631, 0.4074282000805807, 0.3806797345762037, 0.3551849676052772, 0.3299145481664544, 0.3049445501267351, 0.2800466844026235, 0.2574367568132028, 0.24833443135536318, 0.20227555854008525]\n"
     ]
    }
   ],
   "source": [
    "print(model(2000 , 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_gradient(cost, epsilon):\n",
    "\n",
    "def gradient_checking(x, y , parameters ,gradients ,  epsilon = 1e-7):\n",
    "    \n",
    "    grad = gradients_to_vector(gradients)\n",
    "    theta, _ = dictionary_to_vector(parameters)\n",
    "    num_parameters = theta.shape[0]\n",
    "    # gradients = []\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range (num_parameters):\n",
    "        temp_theta = np.copy(theta)\n",
    "        temp_theta[i][0] = temp_theta[i][0] + epsilon\n",
    "        temp_parameters = vector_to_dictionary(temp_theta)\n",
    "        _ , J_plus[i] = forward_prop(x,y, temp_parameters)\n",
    "\n",
    "        temp_theta = np.copy(theta)\n",
    "        temp_theta[i] = temp_theta[i] - epsilon\n",
    "        temp_parameters = vector_to_dictionary(temp_theta)\n",
    "        _ , J_minus[i] = forward_prop(x,y, temp_parameters)\n",
    "\n",
    "        gradapprox[i]= (J_plus[i] - J_minus[i])/(2*epsilon)\n",
    "        \n",
    "\n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    dinominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "\n",
    "    difference = numerator/dinominator\n",
    "\n",
    "    if difference>2e-7:\n",
    "        print(\"there is a problem in back propagation algorithm\")\n",
    "    else:\n",
    "        print(\"the back propagation is correctly implemented\")\n",
    "\n",
    "    return difference\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a problem in back propagation algorithm\n",
      "0.4484032978823968 diff\n"
     ]
    }
   ],
   "source": [
    "x, y, parameters = gradient_check_n_test_case()\n",
    "cache , cost = forward_prop(x, y, parameters)\n",
    "gradients = backprop(x,y, parameters, cache)\n",
    "\n",
    "diff = gradient_checking(x, y, parameters, gradients)\n",
    "print(diff , \"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
